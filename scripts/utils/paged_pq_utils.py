import torch
from typing import Dict
from .pq_utils import sa_encode_4d_keops, sa_decode_4d, DynamicPQCache
from .Timer import Timer
import logging

logger = logging.getLogger(__name__)


class PagedPQCache(DynamicPQCache):
    """
    ç®€åŒ–çš„é¡µå¼PQç¼“å­˜ï¼Œé‡‡ç”¨è¿ç»­å­˜å‚¨ç­–ç•¥
    
    ä¸»è¦ç‰¹æ€§ï¼š
    1. æ‰©å±•æ®‹å·®ç¼“å­˜ä»64åˆ°128 tokens  
    2. VçŸ©é˜µé‡‡ç”¨æŒ‰é¡µè½¬ç½®çš„è¿ç»­å­˜å‚¨
    3. ç®€åŒ–å†…å­˜ç®¡ç†ï¼Œæ— éœ€åŠ¨æ€é¡µé¢åˆ†é…
    """
    
    def __init__(self, *, bs, nh, num_key_value_heads, M, layer_num, 
                 dtype=torch.uint8, nbits=8, d=128, scalar_t=torch.float32,
                 page_size=64, extended_residual_size=128, max_pages_per_layer=None):
        """
        åˆå§‹åŒ–ç®€åŒ–çš„é¡µå¼PQç¼“å­˜
        
        Args:
            page_size: é¡µé¢å¤§å°ï¼ˆtokensï¼‰ï¼Œé»˜è®¤64ï¼Œä»…ç”¨äºè½¬ç½®é€»è¾‘
            extended_residual_size: æ‰©å±•æ®‹å·®ç¼“å­˜å¤§å°ï¼Œé»˜è®¤128
            max_pages_per_layer: ä¿ç•™å‚æ•°ï¼Œå…¼å®¹æ€§è€ƒè™‘
        """
        # é¡µå¼è½¬ç½®å‚æ•°
        self.page_size = page_size  # 64 tokens per pageï¼Œä»…ç”¨äºè½¬ç½®é€»è¾‘
        self.extended_residual_size = extended_residual_size  # 128 tokens

        # æ‰‹åŠ¨è®¾ç½®çˆ¶ç±»çš„å±æ€§ï¼Œé¿å…è°ƒç”¨init_cache
        self.bs = bs
        self.nh = nh
        self.num_key_value_heads = num_key_value_heads
        self.M = M
        self.layer_num = layer_num
        self.dtype = dtype
        self.nbits = nbits
        self.d = d
        self.scalar_t = scalar_t
        
        # è®¾ç½®çˆ¶ç±»çš„å…¶ä»–å±æ€§
        self.max_residual_length = extended_residual_size  # ç›´æ¥è®¾ç½®ä¸ºæ‰©å±•å¤§å°
        
        # å¯¼å…¥KernelRegistry
        from .pq_utils import KernelRegistry
        self.registery = KernelRegistry(M=M, d=d, nbits=nbits, nh=nh, scalar_t=scalar_t)
        
        # é‡æ–°åˆå§‹åŒ–ç¼“å­˜ï¼ˆè¦†ç›–çˆ¶ç±»çš„åˆå§‹åŒ–ï¼‰
        self.init_cache()
        
        logger.info(f"PagedPQCache initialized with contiguous storage: "
                   f"{layer_num} layers, {extended_residual_size} residual tokens, "
                   f"page_size={page_size} (for transpose logic only)")
        
        # æ£€æŸ¥é¡µå¼å†…æ ¸å¯ç”¨æ€§
        # self._check_paged_kernel_availability()
        
        logger.info(f"PagedPQCache initialized: {layer_num} layers, {extended_residual_size} residual tokens, "
                   f"{page_size} tokens/page")
        
    
    
    def init_cache(self):
        """åˆå§‹åŒ–ç¼“å­˜ç»“æ„"""
        # KçŸ©é˜µï¼šä½¿ç”¨ä¼ ç»Ÿå­˜å‚¨ï¼ˆrow-wiseï¼‰ï¼Œä¼˜åŒ–attentionè®¡ç®—
        self.key_cache = [
            torch.zeros((self.bs, self.num_key_value_heads, 0, self.M), dtype=self.dtype, device='cuda')
            for _ in range(self.layer_num)
        ]
        
        # VçŸ©é˜µï¼šä½¿ç”¨è½¬ç½®å­˜å‚¨æ ¼å¼ (bs, nh_k, M, 0)
        self.value_cache = [
            torch.zeros((self.bs, self.num_key_value_heads, self.M, 0), dtype=self.dtype, device='cuda')
            for _ in range(self.layer_num)
        ]
        
        # æ”¹è¿›ï¼šä¼˜åŒ–æ®‹å·®ç¼“å­˜å¤§å°ï¼Œæ ¹æ®å®é™…ä½¿ç”¨åœºæ™¯åŠ¨æ€è°ƒæ•´
        # é¢„å¡«å……é˜¶æ®µï¼šä½¿ç”¨è¾ƒå°çš„æ®‹å·®ç¼“å­˜ï¼ˆ64 tokensï¼‰
        # è§£ç é˜¶æ®µï¼šä½¿ç”¨è¾ƒå¤§çš„æ®‹å·®ç¼“å­˜ï¼ˆ128 tokensï¼‰
        prefill_residual_size = self.page_size  # 64 tokens
        decoding_residual_size = self.extended_residual_size  # 128 tokens
        
        # é¢„å¡«å……æ®‹å·®ç¼“å­˜ï¼ˆè¾ƒå°ï¼ŒèŠ‚çœå†…å­˜ï¼‰
        self.key_prefill_residual = [
            torch.zeros((self.bs, self.num_key_value_heads, prefill_residual_size, self.d), 
                       dtype=self.scalar_t, device='cuda')
            for _ in range(self.layer_num)
        ]
        
        self.value_prefill_residual = [
            torch.zeros((self.bs, self.num_key_value_heads, prefill_residual_size, self.d), 
                       dtype=self.scalar_t, device='cuda')
            for _ in range(self.layer_num)
        ]
        
        # è§£ç æ®‹å·®ç¼“å­˜ï¼ˆè¾ƒå¤§ï¼Œæ”¯æŒé•¿åºåˆ—ï¼‰
        self.key_residual_cache = [
            torch.zeros((self.bs, self.num_key_value_heads, decoding_residual_size, self.d), 
                       dtype=self.scalar_t, device='cuda')
            for _ in range(self.layer_num)
        ]
        
        self.value_residual_cache = [
            torch.zeros((self.bs, self.num_key_value_heads, decoding_residual_size, self.d), 
                       dtype=self.scalar_t, device='cuda')
            for _ in range(self.layer_num)
        ]
        
        # é‡ç½®è®¡æ•°å™¨
        self.seen_tokens = [0 for _ in range(self.layer_num)]
        self.residualed_tokens = [0 for _ in range(self.layer_num)]
        
        # æ”¹è¿›ï¼šæ·»åŠ å†…å­˜ä½¿ç”¨ç»Ÿè®¡
        total_memory_mb = (
            sum(cache.numel() * cache.element_size() for cache in self.key_cache + self.value_cache) +
            sum(cache.numel() * cache.element_size() for cache in self.key_prefill_residual + self.value_prefill_residual) +
            sum(cache.numel() * cache.element_size() for cache in self.key_residual_cache + self.value_residual_cache)
        ) / (1024 * 1024)
        
        logger.info(f"PagedPQCache memory initialized: {total_memory_mb:.2f} MB "
                   f"(K_traditional: {len(self.key_cache)} layers, "
                   f"V_contiguous: {len(self.value_cache)} layers, "
                   f"residual: {prefill_residual_size + decoding_residual_size} tokens)")
    
    def flush_to_pages(self, layer_idx: int):
        """
        å°†æ®‹å·®ç¼“å­˜çš„å‰64ä¸ªtoken flushåˆ°å­˜å‚¨
        
        æ”¹è¿›ï¼šKçŸ©é˜µä½¿ç”¨ä¼ ç»Ÿå­˜å‚¨ï¼ŒVçŸ©é˜µä½¿ç”¨æŒ‰é¡µè½¬ç½®åè¿ç»­å­˜å‚¨
        
        Args:
            layer_idx: å±‚ç´¢å¼•
        """
        logger.info(f"flush_to_pagesè¢«è°ƒç”¨: layer_idx={layer_idx}, residualed_tokens={self.residualed_tokens[layer_idx]}, page_size={self.page_size}")
        
        if self.residualed_tokens[layer_idx] < self.page_size:
            logger.debug(f"Layer {layer_idx}: Not enough tokens to flush ({self.residualed_tokens[layer_idx]}/{self.page_size})")
            return  # ä¸è¶³64ä¸ªtokenï¼Œæ— éœ€flush
        
        logger.debug(f"Layer {layer_idx}: Flushing {self.page_size} tokens to pages")
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        logger.debug(f"flush_to_pageså¼€å§‹æ—¶çš„ç¼“å­˜å½¢çŠ¶:")
        logger.debug(f"  key_residual_cache[{layer_idx}]: {self.key_residual_cache[layer_idx].shape}")
        logger.debug(f"  value_residual_cache[{layer_idx}]: {self.value_residual_cache[layer_idx].shape}")
        
        # æå–å‰64ä¸ªtokençš„KV
        k_to_flush = self.key_residual_cache[layer_idx][:, :, :self.page_size, :]  # (bs, nh_k, 64, d)
        v_to_flush = self.value_residual_cache[layer_idx][:, :, :self.page_size, :]  # (bs, nh_k, 64, d)
        
        logger.debug(f"æå–çš„KVå½¢çŠ¶:")
        logger.debug(f"  k_to_flush: {k_to_flush.shape}")
        logger.debug(f"  v_to_flush: {v_to_flush.shape}")
        
        # æ”¹è¿›ï¼šKçŸ©é˜µä½¿ç”¨ä¼ ç»Ÿå­˜å‚¨ï¼ˆrow-wiseï¼‰ï¼Œä¼˜åŒ–attentionè®¡ç®—
        k_codes = sa_encode_4d_keops(k_to_flush, self.key_cent, target_dtype=self.dtype)  # (bs, nh_k, 64, M)
        self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], k_codes], dim=2)
        
        logger.debug(f"KçŸ©é˜µä½¿ç”¨ä¼ ç»Ÿå­˜å‚¨ï¼Œæ›´æ–°åçš„å½¢çŠ¶: {self.key_cache[layer_idx].shape}")
        
        # VçŸ©é˜µï¼šä½¿ç”¨è¿ç»­å­˜å‚¨ï¼ˆä¸Kä¸€è‡´ï¼‰ï¼ŒæŒ‰é¡µè½¬ç½®åå­˜å‚¨
        v_codes = sa_encode_4d_keops(v_to_flush, self.value_cent, target_dtype=self.dtype)  # (bs, nh_k, 64, M)
        
        logger.debug(f"Vç¼–ç åçš„å½¢çŠ¶: {v_codes.shape}")
        
        # æŒ‰é¡µè½¬ç½®åè¿ç»­å­˜å‚¨åˆ°value_cache
        # å¯¹æ¯ä¸ªé¡µé¢è¿›è¡Œè½¬ç½®ï¼Œç„¶åæ‹¼æ¥
        transposed_v_codes = v_codes.permute(0, 1, 3, 2).contiguous()  # (bs, nh_k, M, 64)
        # ç›´æ¥å­˜å‚¨è½¬ç½®åçš„æ•°æ®
        self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], transposed_v_codes], dim=3)
        
        logger.debug(f"Layer {layer_idx}: V matrix stored continuously in value_cache "
                   f"(shape: {self.value_cache[layer_idx].shape}) after page-wise transposition")
        
        # ç§»åŠ¨æ®‹å·®ç¼“å­˜ï¼šä¿ç•™åé¢çš„token
        remaining_tokens = self.residualed_tokens[layer_idx] - self.page_size
        
        logger.debug(f"ç§»åŠ¨æ®‹å·®ç¼“å­˜å‰çš„çŠ¶æ€:")
        logger.debug(f"  remaining_tokens: {remaining_tokens}")
        logger.debug(f"  key_residual_cache[{layer_idx}]å½¢çŠ¶: {self.key_residual_cache[layer_idx].shape}")
        logger.debug(f"  value_residual_cache[{layer_idx}]å½¢çŠ¶: {self.value_residual_cache[layer_idx].shape}")
        
        if remaining_tokens > 0:
            # å°†åé¢çš„tokenç§»åˆ°å‰é¢
            # ä½¿ç”¨clone()é¿å…å¼•ç”¨é—®é¢˜
            key_src = self.key_residual_cache[layer_idx][:, :, self.page_size:self.residualed_tokens[layer_idx], :].clone()
            value_src = self.value_residual_cache[layer_idx][:, :, self.page_size:self.residualed_tokens[layer_idx], :].clone()
            
            # æ¸…ç©ºç›®æ ‡åŒºåŸŸ
            self.key_residual_cache[layer_idx][:, :, :remaining_tokens, :].fill_(0)
            self.value_residual_cache[layer_idx][:, :, :remaining_tokens, :].fill_(0)
            
            # å¤åˆ¶æ•°æ®
            self.key_residual_cache[layer_idx][:, :, :remaining_tokens, :] = key_src
            self.value_residual_cache[layer_idx][:, :, :remaining_tokens, :] = value_src
        else:
            # å¦‚æœremaining_tokens <= 0ï¼Œæ¸…ç©ºæ®‹å·®ç¼“å­˜
            self.key_residual_cache[layer_idx].fill_(0)
            self.value_residual_cache[layer_idx].fill_(0)
        
        # æ›´æ–°è®¡æ•°
        self.residualed_tokens[layer_idx] = remaining_tokens
        self.seen_tokens[layer_idx] += self.page_size
        
        logger.debug(f"Layer {layer_idx}: Flush completed. Remaining residual tokens: {remaining_tokens}")
    
    def test_method(self):
        print("DEBUG: PagedPQCache.test_method called")
        return "test"
    
    def prefill(self, query_states, key_states, value_states, layer_idx, distort_recent=False):
        """
        é¢„å¡«å……æ–¹æ³• - ä¼˜åŒ–é¢„å¡«å……é˜¶æ®µçš„å­˜å‚¨
        
        æ”¹è¿›ï¼šKçŸ©é˜µä½¿ç”¨ä¼ ç»Ÿå­˜å‚¨ï¼ˆä¼˜åŒ–attentionè®¡ç®—ï¼‰ï¼ŒVçŸ©é˜µä½¿ç”¨æŒ‰é¡µè½¬ç½®åè¿ç»­å­˜å‚¨ï¼ˆä¼˜åŒ–å†…å­˜è®¿é—®ï¼‰
        
        Args:
            query_states: Queryå¼ é‡ (bs, nh, prefill_length, d)
            key_states: Keyå¼ é‡ (bs, nh_k, prefill_length, d)
            value_states: Valueå¼ é‡ (bs, nh_k, prefill_length, d)
            layer_idx: å±‚ç´¢å¼•
            distort_recent: æ˜¯å¦åœ¨prefillé˜¶æ®µä½¿ç”¨é‡åŒ–æ•°æ®
            
        Returns:
            attention_output: æ³¨æ„åŠ›è¾“å‡º
        """
        
        # æ ‡è®°å½“å‰å¤„äºprefillé˜¶æ®µï¼Œé¿å…é¡µé¢è¦†ç›–
        self._is_prefill_phase = True
        
        with Timer("PagedPQCache.prefill"):
            prefill_length = key_states.size(2)
            
            # æ€»æ˜¯è¿›è¡Œé‡åŒ–ç¼–ç å’Œå­˜å‚¨
            with Timer("PagedPQCache.prefill.encode"):
                key_codes = sa_encode_4d_keops(key_states, self.key_cent, target_dtype=self.dtype)
                value_codes = sa_encode_4d_keops(value_states, self.value_cent, target_dtype=self.dtype)
                # å‡å°‘åŒæ­¥è°ƒç”¨ï¼Œåªåœ¨å¿…è¦æ—¶åŒæ­¥
                # torch.cuda.synchronize()
            
            # æ”¹è¿›ï¼šKçŸ©é˜µä½¿ç”¨ä¼ ç»Ÿå­˜å‚¨ï¼ˆrow-wiseï¼‰ï¼Œä¼˜åŒ–attentionè®¡ç®—
            with Timer("PagedPQCache.prefill.store_k_traditional"):
                # KçŸ©é˜µç›´æ¥å­˜å‚¨åˆ°key_cacheï¼Œä¿æŒä¼ ç»Ÿæ ¼å¼
                self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_codes], dim=2)
                # å‡å°‘åŒæ­¥è°ƒç”¨
                # torch.cuda.synchronize()
                
                logger.info(f"Layer {layer_idx}: K matrix stored traditionally in key_cache "
                           f"(shape: {self.key_cache[layer_idx].shape})")
            
            # import ipdb; ipdb.set_trace()
            # VçŸ©é˜µï¼šæŒ‰é¡µè½¬ç½®åï¼Œè¿ç»­å­˜å‚¨ï¼ˆä¸Kä¸€è‡´ï¼‰
            with Timer("PagedPQCache.prefill.store_v_contiguous"):
                # æŒ‰é¡µç²’åº¦è¿›è¡Œè½¬ç½®ï¼Œç„¶åè¿ç»­å­˜å‚¨
                batch_size = self.page_size
                num_batches = (prefill_length + batch_size - 1) // batch_size
                
                # å­˜å‚¨è½¬ç½®åçš„æ‰€æœ‰chunk
                transposed_chunks = []
                
                for batch_idx in range(num_batches):
                    start_idx = batch_idx * batch_size
                    end_idx = min(start_idx + batch_size, prefill_length)
                    chunk = value_codes[:, :, start_idx:end_idx, :]  # (bs, nh_k, cur_page_tokens, M)
                    
                    # é¡µå†…è½¬ç½®ï¼š(bs, nh_k, cur_page_tokens, M) -> (bs, nh_k, M, cur_page_tokens)
                    transposed_chunk = chunk.permute(0, 1, 3, 2).contiguous()  # (bs, nh_k, M, cur_page_tokens)
                    
                    # # ä¸ºäº†ä¸Kä¿æŒä¸€è‡´çš„å­˜å‚¨æ ¼å¼ï¼Œå†è½¬å›ï¼š(bs, nh_k, M, cur_page_tokens) -> (bs, nh_k, cur_page_tokens, M)
                    # final_chunk = transposed_chunk.permute(0, 1, 3, 2).contiguous()  # (bs, nh_k, cur_page_tokens, M)
                    
                    transposed_chunks.append(transposed_chunk)
                
                # è¿ç»­å­˜å‚¨æ‰€æœ‰è½¬ç½®åçš„chunk
                if transposed_chunks:
                    # è½¬ç½®åçš„chunkå½¢çŠ¶æ˜¯ (bs, nh_k, M, cur_page_tokens)
                    # ç›´æ¥æ‹¼æ¥è½¬ç½®åçš„æ•°æ®ï¼Œä¿æŒè½¬ç½®æ ¼å¼
                    all_transposed = torch.cat(transposed_chunks, dim=3)  # (bs, nh_k, M, total_tokens)
                    self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], all_transposed], dim=3)
                
                logger.info(f"Layer {layer_idx}: V matrix stored contiguously after page-wise transposition "
                           f"(shape: {self.value_cache[layer_idx].shape}, {num_batches} pages processed)")

            # å¦‚æœéœ€è¦distort_recentï¼Œåé‡åŒ–ç”¨äºattentionè®¡ç®—
            if distort_recent:
                with Timer("PagedPQCache.prefill.decode"):
                    key_states = sa_decode_4d(key_codes, self.key_cent)
                    value_states = sa_decode_4d(value_codes, self.value_cent)
                    
            
            # æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—
            with Timer("PagedPQCache.prefill.attention"):
                from torch.nn.functional import scaled_dot_product_attention as sdpa
                from transformers.models.llama.modeling_llama import repeat_kv

                num_heads = query_states.size(1)
                num_kv_heads = key_states.size(1)
                nrep = num_heads // num_kv_heads

                key_states = repeat_kv(key_states, nrep)
                value_states = repeat_kv(value_states, nrep)
                
                attention_output = sdpa(query_states, key_states, value_states, is_causal=True)
                
                logger.debug(f"PagedPQCache prefill completed: layer={layer_idx}, "
                           f"K_traditional={self.key_cache[layer_idx].size(2)} tokens, "
                           f"V_transposed={self.value_cache[layer_idx].size(3)} tokens")
                
                # æ›´æ–°seen_tokensè®¡æ•°
                self.seen_tokens[layer_idx] += prefill_length
        
                # æ¸…é™¤prefillé˜¶æ®µæ ‡è®°
                self._is_prefill_phase = False
                
                return attention_output
    
    def update(self, key_states, value_states, layer_idx, distort_recent=False):
        """
        é‡å†™çˆ¶ç±»çš„updateæ–¹æ³•ï¼Œç¡®ä¿æ­£ç¡®ç®¡ç†å­˜å‚¨
        
        Args:
            key_states: Keyå¼ é‡ (bs, nh_k, update_length, d)
            value_states: Valueå¼ é‡ (bs, nh_k, update_length, d)
            layer_idx: å±‚ç´¢å¼•
            distort_recent: æ˜¯å¦åœ¨prefillé˜¶æ®µä½¿ç”¨é‡åŒ–æ•°æ®
            
        Returns:
            key_states, value_states: æ›´æ–°åçš„KVçŠ¶æ€
        """
        # ç›´æ¥è°ƒç”¨çˆ¶ç±»çš„updateæ–¹æ³•ï¼Œå› ä¸ºprefillé˜¶æ®µç°åœ¨ç›´æ¥è¿›è¡Œå­˜å‚¨
        # ä¸éœ€è¦å¤æ‚çš„æ®‹å·®ç¼“å­˜ç®¡ç†
        key_states, value_states = super().update(key_states, value_states, layer_idx, distort_recent)
        
        return key_states, value_states
    
    def decoding_with_pages(self, query_states, key_states, value_states, layer_idx):
        """
        ä½¿ç”¨å­˜å‚¨çš„è§£ç attention - è°ƒç”¨çœŸæ­£çš„é¡µå¼CUDAå†…æ ¸
        
        Args:
            query_states: Queryå¼ é‡ (bs, nh, 1, d)
            key_states: Keyå¼ é‡ (bs, nh_k, 1, d)  
            value_states: Valueå¼ é‡ (bs, nh_k, 1, d)
            layer_idx: å±‚ç´¢å¼•
            
        Returns:
            attention_output: æ³¨æ„åŠ›è¾“å‡º
        """
        # ç¡®ä¿å½“å‰å¤„äºdecodingé˜¶æ®µ
        self._is_prefill_phase = False
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦flush
            if self.residualed_tokens[layer_idx] >= self.extended_residual_size:
                logger.debug(f"Layer {layer_idx}: Residual cache full, flushing to pages")
                self.flush_to_pages(layer_idx)
            
            # æ·»åŠ æ–°tokenåˆ°æ®‹å·®ç¼“å­˜
            r = self.residualed_tokens[layer_idx]
            n = key_states.size(2)  # æ–°tokenæ•°é‡
            
            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ®‹å·®ç¼“å­˜æœ‰è¶³å¤Ÿç©ºé—´
            if r + n > self.extended_residual_size:
                logger.warning(f"Layer {layer_idx}: Residual cache overflow detected: {r + n} > {self.extended_residual_size}. Force flushing.")
                self.flush_to_pages(layer_idx)
                r = self.residualed_tokens[layer_idx]
            
            # å®‰å…¨æ£€æŸ¥ï¼šæ®‹å·®ç¼“å­˜çŠ¶æ€
            if r + n > self.extended_residual_size:
                logger.warning(f"Layer {layer_idx}: Residual cache near full: {r + n}/{self.extended_residual_size}")
            
            self.key_residual_cache[layer_idx][:, :, r:r+n, :] = key_states
            self.value_residual_cache[layer_idx][:, :, r:r+n, :] = value_states
            self.residualed_tokens[layer_idx] += n
            self.seen_tokens[layer_idx] += n
            
            logger.debug(f"Layer {layer_idx}: Added {n} tokens, total residual: {self.residualed_tokens[layer_idx]}")
            
            # è°ƒç”¨çœŸæ­£çš„é¡µå¼CUDAå†…æ ¸
            logger.info(f"Layer {layer_idx}: ğŸ¯ About to call paged kernel with residual_length={r + n}")
            return self._call_paged_kernel(query_states, layer_idx, r + n)
            
        except Exception as e:
            logger.error(f"PagedPQCache decoding failed at layer {layer_idx}: {e}. Falling back to standard decoding.")
            # å¦‚æœé¡µå¼å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†å¤„ç†
            # ç¡®ä¿çŠ¶æ€ä¸€è‡´æ€§
            try:
                self.seen_tokens[layer_idx] += key_states.size(2)
                return super().decoding(query_states, key_states, value_states, layer_idx)
            except Exception as fallback_e:
                logger.critical(f"Fallback decoding also failed: {fallback_e}")
                raise
    
    def _call_paged_kernel(self, query_states, layer_idx, residual_length):
        """
        è°ƒç”¨é¡µå¼CUDAå†…æ ¸è¿›è¡Œattentionè®¡ç®— - é€‚é…è¿ç»­å­˜å‚¨çš„VçŸ©é˜µ
        
        Args:
            query_states: Queryå¼ é‡ (bs, nh, 1, d)
            layer_idx: å±‚ç´¢å¼•
            residual_length: æ®‹å·®ç¼“å­˜é•¿åº¦
            
        Returns:
            attention_output: æ³¨æ„åŠ›è¾“å‡º
        """
        try:
            # è·å–å½“å‰å±‚çš„ç¼“å­˜æ•°æ®
            current_device = query_states.device
            bs = query_states.size(0)
            nh = query_states.size(1)
            nh_k = self.num_key_value_heads
            
            # å‡†å¤‡Keyæ•°æ®ï¼šé‡åŒ–ç¼“å­˜ + æ®‹å·®ç¼“å­˜
            key_codes = self.key_cache[layer_idx].to(current_device)  # (bs, nh_k, nk, M)
            key_cents = self.key_cent.to(current_device)  # (M, C, d/M)
            key_residuals = self.key_residual_cache[layer_idx][:, :, :residual_length, :].to(current_device)  # (bs, nh_k, r, d)
            
            # å‡†å¤‡Væ•°æ®ï¼šè¿ç»­å­˜å‚¨çš„è½¬ç½®ç¼“å­˜ + æ®‹å·®ç¼“å­˜
            value_codes = self.value_cache[layer_idx].to(current_device)  # (bs, nh_k, M, tokens) - è¿ç»­å­˜å‚¨çš„è½¬ç½®æ ¼å¼
            value_cents = self.value_cent.to(current_device)  # (M, C, d/M)
            value_residuals = self.value_residual_cache[layer_idx][:, :, :residual_length, :].to(current_device)  # (bs, nh_k, r, d)
            
            # è®¡ç®—é¡µé¢ç›¸å…³å‚æ•°
            total_tokens = value_codes.size(3)  # æ€»tokenæ•°é‡
            n_pages = (total_tokens + self.page_size - 1) // self.page_size  # å‘ä¸Šå–æ•´
            page_size = self.page_size
            
            # ç¡®ä¿é¡µé¢æ•°é‡ä¸é¡µé¢æ± åŒ¹é…
            if n_pages == 0:
                logger.warning(f"Layer {layer_idx}: No pages available, using fallback")
                return self._fallback_to_standard_decoding(query_states, layer_idx, residual_length)
            
            # æ„å»ºè™šæ‹Ÿé¡µé¢IDï¼ˆè¿ç»­å­˜å‚¨ï¼Œé¡µé¢IDå°±æ˜¯è¿ç»­çš„ï¼‰
            # ç¡®ä¿é¡µé¢IDåœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼Œé¿å…å†…æ ¸è®¿é—®è¶Šç•Œ
            value_page_ids = torch.arange(n_pages, dtype=torch.int64, device=current_device)
            value_page_ids = value_page_ids.unsqueeze(0).unsqueeze(0).expand(bs, nh_k, -1)  # (bs, nh_k, n_pages)
            
            logger.debug(f"Layer {layer_idx}: value_page_ids shape: {value_page_ids.shape}, max_page_id: {value_page_ids.max()}")
            
            logger.debug(f"Layer {layer_idx}: value_page_ids shape: {value_page_ids.shape}, max_page_id: {value_page_ids.max()}")
            
            # æ„å»ºé¡µé¢æ±  - ç›´æ¥reshapeè¿ç»­å­˜å‚¨çš„Væ•°æ®
            # Vçš„å½“å‰å½¢çŠ¶ï¼š(bs, nh_k, M, tokens) - è¿ç»­å­˜å‚¨çš„è½¬ç½®æ ¼å¼
            # é¡µå¼å†…æ ¸æœŸæœ›çš„å½¢çŠ¶ï¼š(max_pages, M, page_size)
            
            # è®¡ç®—å®é™…éœ€è¦çš„é¡µé¢æ•°ï¼ˆå‘ä¸Šå–æ•´ï¼‰
            actual_pages_needed = (total_tokens + page_size - 1) // page_size
            max_pages = actual_pages_needed
            
            # ç›´æ¥reshape Væ•°æ®ä»¥é€‚é…é¡µå¼å†…æ ¸æ¥å£
            # å–ç¬¬ä¸€ä¸ªbatchå’Œheadçš„æ•°æ®ï¼š(M, tokens)
            v_data = value_codes[0, 0, :, :]  # (M, tokens)
            
            # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            v_data = v_data.to(current_device)
            
            logger.debug(f"Layer {layer_idx}: V data shape: {v_data.shape}, total_tokens: {total_tokens}, page_size: {page_size}")
            
            # å¦‚æœtokensæ•°é‡ä¸æ˜¯page_sizeçš„æ•´æ•°å€ï¼Œéœ€è¦padding
            if total_tokens % page_size != 0:
                padding_size = page_size - (total_tokens % page_size)
                # åœ¨æœ€åä¸€ä¸ªç»´åº¦æ·»åŠ padding
                v_data = torch.cat([v_data, torch.zeros((self.M, padding_size), dtype=self.dtype, device=current_device)], dim=1)
                total_tokens_padded = total_tokens + padding_size
                logger.debug(f"Layer {layer_idx}: Added padding: {padding_size}, new shape: {v_data.shape}")
            else:
                total_tokens_padded = total_tokens
            
            # Reshapeä¸ºé¡µå¼å†…æ ¸æœŸæœ›çš„å½¢çŠ¶ï¼š(M, tokens) -> (M, pages, page_size) -> (pages, M, page_size)
            # æ³¨æ„ï¼štokensç»´åº¦éœ€è¦é‡æ–°ç»„ç»‡ä¸º(pages, page_size)
            try:
                # ç¡®ä¿æ•°æ®å¤§å°æ­£ç¡®
                expected_size = self.M * max_pages * page_size
                actual_size = v_data.numel()
                
                if actual_size != expected_size:
                    logger.error(f"Layer {layer_idx}: Data size mismatch! expected={expected_size}, actual={actual_size}")
                    logger.error(f"Layer {layer_idx}: v_data.shape={v_data.shape}, max_pages={max_pages}, page_size={page_size}")
                    raise ValueError(f"Data size mismatch: {actual_size} != {expected_size}")
                
                v_data_reshaped = v_data.view(self.M, max_pages, page_size).transpose(0, 1).contiguous()  # (pages, M, page_size)
                logger.debug(f"Layer {layer_idx}: Reshaped V data: {v_data_reshaped.shape}")
                
                # éªŒè¯reshapeç»“æœ
                if v_data_reshaped.shape != (max_pages, self.M, page_size):
                    logger.error(f"Layer {layer_idx}: Reshape result shape mismatch! expected=({max_pages}, {self.M}, {page_size}), actual={v_data_reshaped.shape}")
                    raise ValueError(f"Reshape result shape mismatch: {v_data_reshaped.shape}")
                    
            except Exception as e:
                logger.error(f"Layer {layer_idx}: Reshape failed: {e}")
                logger.error(f"Layer {layer_idx}: v_data.shape={v_data.shape}, max_pages={max_pages}, page_size={page_size}")
                raise
            
            # åˆ›å»ºé¡µé¢æ± å¼ é‡
            value_page_pool = v_data_reshaped  # (max_pages, M, page_size)
            
            # éªŒè¯é¡µé¢æ± çš„æœ‰æ•ˆæ€§
            if value_page_pool.shape[0] != max_pages:
                logger.error(f"Layer {layer_idx}: Page pool shape mismatch! expected_pages={max_pages}, actual_pages={value_page_pool.shape[0]}")
                raise ValueError(f"Page pool shape mismatch: {value_page_pool.shape[0]} != {max_pages}")
            
            # éªŒè¯é¡µé¢IDçš„æœ‰æ•ˆæ€§
            if value_page_ids.max() >= max_pages:
                logger.error(f"Layer {layer_idx}: Invalid page IDs detected! max_page_id={value_page_ids.max()}, max_pages={max_pages}")
                raise ValueError(f"Page ID out of bounds: {value_page_ids.max()} >= {max_pages}")
            
            logger.debug(f"Layer {layer_idx}: value_page_pool shape: {value_page_pool.shape}, dtype: {value_page_pool.dtype}")
            
            # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            if value_page_pool.dtype != torch.uint8:
                logger.warning(f"Layer {layer_idx}: Converting value_page_pool from {value_page_pool.dtype} to uint8")
                value_page_pool = value_page_pool.to(torch.uint8)
            
            # è·å–é¢„åˆ†é…çš„ç¼“å†²åŒº
            kernel_registry = self.registery
            l = self.seen_tokens[layer_idx]
            
            # ä»pq_utilså¯¼å…¥l2Nså‡½æ•°
            from .pq_utils import l2Ns
            Ns = l2Ns(l)
            
            # ç¡®ä¿å¿…è¦çš„ç¼“å†²åŒºå·²ç»å­˜åœ¨
            if Ns not in kernel_registry.partial_out_buffers:
                logger.warning(f"Layer {layer_idx}: Buffer for Ns={Ns} not found, creating it")
                # åˆ›å»ºå¿…è¦çš„ç¼“å†²åŒº
                bs = 1  # typical for inference
                nh = self.nh
                d = self.d
                scalar_t = self.scalar_t
                
                partial_out_buffer = torch.empty(bs, nh, Ns+1, d, dtype=scalar_t, device='cuda')
                partial_lse_buffer = torch.empty(bs, nh, Ns+1, dtype=scalar_t, device='cuda')
                
                kernel_registry.partial_out_buffers[Ns] = partial_out_buffer
                kernel_registry.partial_lse_buffers[Ns] = partial_lse_buffer
            
            partial_out_buffer = kernel_registry.partial_out_buffers[Ns]
            partial_lse_buffer = kernel_registry.partial_lse_buffers[Ns]
            
            # æ„å»ºé¡µå¼å†…æ ¸å‡½æ•°å
            # æ ¹æ®å½“å‰å‚æ•°é€‰æ‹©åˆé€‚çš„å†…æ ¸
            paged_func_name = f"flash_decoding_paged_v_f16u8_Ns{Ns}Lt{self.extended_residual_size}d{self.d}M{self.M}C256"
            
            logger.info(f"Layer {layer_idx}: ğŸš€ Calling paged kernel: {paged_func_name}")
            
            try:
                # å¯¼å…¥bindingså¹¶è·å–é¡µå¼å†…æ ¸å‡½æ•°
                import sys
                import importlib.util
                from pathlib import Path
                
                # è·å–é¡¹ç›®æ ¹ç›®å½•
                current_file = Path(__file__).resolve()
                project_root = current_file.parent
                while project_root.parent != project_root:
                    if (project_root / "scripts").exists():
                        break
                    project_root = project_root.parent
                
                local_bindings_path = str(project_root / "scripts" / "modeldb" / "bindings")
                local_bindings_so = Path(local_bindings_path) / "bindings.cpython-312-x86_64-linux-gnu.so"
                
                if not local_bindings_so.exists():
                    raise FileNotFoundError(f"Local bindings.so not found: {local_bindings_so}")
                
                # å¼ºåˆ¶åŠ è½½æœ¬åœ°bindingsæ¨¡å—
                spec = importlib.util.spec_from_file_location("bindings", local_bindings_so)
                local_bindings = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(local_bindings)
                
                logger.info(f"âœ… Forced loading local bindings from: {local_bindings_so}")
                
                # éªŒè¯å†…æ ¸å‡½æ•°æ˜¯å¦å­˜åœ¨
                if not hasattr(local_bindings, paged_func_name):
                    logger.warning(f"Layer {layer_idx}: Paged kernel {paged_func_name} not found, trying alternative kernels")
                    # å°è¯•æ‰¾åˆ°å¯ç”¨çš„é¡µå¼å†…æ ¸
                    available_kernels = [f for f in dir(local_bindings) if 'flash_decoding_paged' in f]
                    if available_kernels:
                        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„é¡µå¼å†…æ ¸
                        paged_func_name = available_kernels[0]
                        logger.info(f"Layer {layer_idx}: Using alternative kernel: {paged_func_name}")
                    else:
                        raise AttributeError("No paged kernels available")
                
                paged_kernel_func = getattr(local_bindings, paged_func_name)
                
                # è¾“å…¥æ•°æ®å¥åº·æ£€æŸ¥
                def check_tensor_health(tensor, name):
                    if torch.isnan(tensor).any():
                        logger.error(f"Layer {layer_idx}: {name} contains NaN values")
                        return False
                    if torch.isinf(tensor).any():
                        logger.error(f"Layer {layer_idx}: {name} contains Inf values")
                        return False
                    return True
                
                # æ£€æŸ¥æ‰€æœ‰è¾“å…¥å¼ é‡
                input_health_checks = [
                    check_tensor_health(query_states, "query_states"),
                    check_tensor_health(key_codes, "key_codes"),
                    check_tensor_health(key_cents, "key_cents"),
                    check_tensor_health(key_residuals, "key_residuals"),
                    check_tensor_health(value_page_ids, "value_page_ids"),
                    check_tensor_health(value_page_pool, "value_page_pool"),
                    check_tensor_health(value_cents, "value_cents"),
                    check_tensor_health(value_residuals, "value_residuals")
                ]
                
                if not all(input_health_checks):
                    logger.error(f"Layer {layer_idx}: Input data health check failed")
                    logger.info(f"Layer {layer_idx}: Falling back to standard decoding due to input data issues")
                    return self._fallback_to_standard_decoding(query_states, layer_idx, residual_length)
                
                # è°ƒç”¨é¡µå¼å†…æ ¸ - æŒ‰ç…§Interface.cuä¸­çš„å‚æ•°é¡ºåº
                try:
                    attn_output = paged_kernel_func(
                        query_states,  # (bs, nh, 1, d)
                        key_codes,     # (bs, nh_k, nk, M)
                        key_cents,     # (M, C, d/M)
                        key_residuals, # (bs, nh_k, r, d)
                        value_page_ids, # (bs, nh_k, n_pages)
                        value_page_pool, # (max_pages, M, page_size)
                        value_cents,   # (M, C, d/M)
                        value_residuals, # (bs, nh_k, r, d)
                        residual_length, # r
                        n_pages,       # number of pages per batch-head
                        page_size,     # tokens per page
                        partial_out_buffer,  # partial_out_buffer
                        partial_lse_buffer   # partial_lse_buffer
                    )
                except RuntimeError as e:
                    if "illegal memory access" in str(e):
                        logger.error(f"Layer {layer_idx}: CUDA memory access error in paged kernel: {e}")
                        logger.error(f"Layer {layer_idx}: This suggests page pool or page ID access issues")
                        return self._fallback_to_standard_decoding(query_states, layer_idx, residual_length)
                    else:
                        raise
                
                # æ£€æŸ¥è¾“å‡ºç»“æœæ˜¯å¦åŒ…å«NaNå€¼
                if torch.isnan(attn_output).any():
                    nan_count = torch.isnan(attn_output).sum().item()
                    total_elements = attn_output.numel()
                    nan_percentage = (nan_count / total_elements) * 100
                    
                    logger.warning(f"Layer {layer_idx}: âš ï¸  PAGED KERNEL PRODUCED NaN VALUES")
                    logger.warning(f"  NaN count: {nan_count} / {total_elements} ({nan_percentage:.4f}%)")
                    
                    # æ˜¾ç¤ºå‰å‡ ä¸ªNaNä½ç½®
                    nan_positions = torch.where(torch.isnan(attn_output))
                    if len(nan_positions[0]) > 0:
                        first_nan_positions = []
                        for i in range(min(5, len(nan_positions[0]))):
                            pos = [nan_positions[j][i].item() for j in range(len(nan_positions))]
                            first_nan_positions.append(pos)
                        logger.warning(f"  First few NaN positions: {first_nan_positions}")
                    
                    logger.warning(f"Layer {layer_idx}: Preserving NaN output for debugging purposes")
                    logger.warning(f"  NOTE: This NaN output will propagate to subsequent layers")
                    logger.warning(f"  RECOMMENDATION: Use CUDA_LAUNCH_BLOCKING=1 for detailed kernel debugging")
                    
                    # è¿”å›NaNè¾“å‡ºï¼Œè®©ä¸Šå±‚å¤„ç†
                    return attn_output
                
                logger.info(f"Layer {layer_idx}: Paged kernel executed successfully")
                return attn_output
                
            except (AttributeError, ImportError) as e:
                logger.warning(f"Layer {layer_idx}: Paged kernel {paged_func_name} not available: {e}")
                return self._fallback_to_standard_decoding(query_states, layer_idx, residual_length)
                
        except Exception as e:
            logger.error(f"Layer {layer_idx}: Failed to call paged kernel: {e}")
            logger.error(f"Layer {layer_idx}: Stack trace:", exc_info=True)
            
            # æœ€åçš„fallback
            return self._fallback_to_standard_decoding(query_states, layer_idx, residual_length)
    
    def _call_continuous_kernel(self, query_states, layer_idx, residual_length):
        """
        ä½¿ç”¨è¿ç»­å­˜å‚¨çš„æ ‡å‡†å†…æ ¸è¿›è¡Œattentionè®¡ç®—
        
        Args:
            query_states: Queryå¼ é‡ (bs, nh, 1, d)
            layer_idx: å±‚ç´¢å¼•
            residual_length: æ®‹å·®ç¼“å­˜é•¿åº¦
            
        Returns:
            attention_output: æ³¨æ„åŠ›è¾“å‡º
        """
        try:
            # è·å–å½“å‰å±‚çš„ç¼“å­˜æ•°æ®
            current_device = query_states.device
            bs = query_states.size(0)
            nh = query_states.size(1)
            nh_k = self.num_key_value_heads
            
            # å‡†å¤‡Keyæ•°æ®ï¼šé‡åŒ–ç¼“å­˜ + æ®‹å·®ç¼“å­˜
            key_codes = self.key_cache[layer_idx].to(current_device)  # (bs, nh_k, nk, M)
            key_cents = self.key_cent.to(current_device)  # (M, C, d/M)
            key_residuals = self.key_residual_cache[layer_idx][:, :, :residual_length, :].to(current_device)  # (bs, nh_k, r, d)
            
            # å‡†å¤‡Væ•°æ®ï¼šè¿ç»­å­˜å‚¨çš„è½¬ç½®ç¼“å­˜ + æ®‹å·®ç¼“å­˜
            value_codes = self.value_cache[layer_idx].to(current_device)  # (bs, nh_k, M, tokens) - è¿ç»­å­˜å‚¨çš„è½¬ç½®æ ¼å¼
            value_cents = self.value_cent.to(current_device)  # (M, C, d/M)
            value_residuals = self.value_residual_cache[layer_idx][:, :, :residual_length, :].to(current_device)  # (bs, nh_k, r, d)
            
            # ä½¿ç”¨æ ‡å‡†çš„allocated_bufferå†…æ ¸ï¼Œä½†ä¼ å…¥è¿ç»­å­˜å‚¨çš„Væ•°æ®
            # éœ€è¦å°†Væ•°æ®è½¬ç½®å›æ ‡å‡†æ ¼å¼
            value_codes_standard = value_codes.transpose(2, 3).contiguous()  # (bs, nh_k, tokens, M)
            
            # è·å–é¢„åˆ†é…çš„ç¼“å†²åŒº
            kernel_registry = self.registery
            l = self.seen_tokens[layer_idx]
            
            # ä»pq_utilså¯¼å…¥l2Nså‡½æ•°
            from .pq_utils import l2Ns
            Ns = l2Ns(l)
            
            # ç¡®ä¿å¿…è¦çš„ç¼“å†²åŒºå·²ç»å­˜åœ¨
            if Ns not in kernel_registry.partial_out_buffers:
                logger.warning(f"Layer {layer_idx}: Buffer for Ns={Ns} not found, creating it")
                # åˆ›å»ºå¿…è¦çš„ç¼“å†²åŒº
                bs = 1  # typical for inference
                nh = self.nh
                d = self.d
                scalar_t = self.scalar_t
                
                partial_out_buffer = torch.empty(bs, nh, Ns+1, d, dtype=scalar_t, device='cuda')
                partial_lse_buffer = torch.empty(bs, nh, Ns+1, dtype=scalar_t, device='cuda')
                
                kernel_registry.partial_out_buffers[Ns] = partial_out_buffer
                kernel_registry.partial_lse_buffers[Ns] = partial_lse_buffer
            
            partial_out_buffer = kernel_registry.partial_out_buffers[Ns]
            partial_lse_buffer = kernel_registry.partial_lse_buffers[Ns]
            
            # æ„å»ºæ ‡å‡†å†…æ ¸å‡½æ•°å
            standard_func_name = f"flash_decoding_allocated_buffer_f16u8_Ns{Ns}Lt{self.extended_residual_size}d{self.d}M{self.M}C256"
            
            logger.debug(f"Layer {layer_idx}: Calling standard kernel: {standard_func_name}")
            
            try:
                # å¯¼å…¥bindingså¹¶è·å–æ ‡å‡†å†…æ ¸å‡½æ•°
                import sys
                import importlib.util
                from pathlib import Path
                
                # è·å–é¡¹ç›®æ ¹ç›®å½•
                current_file = Path(__file__).resolve()
                project_root = current_file.parent
                while project_root.parent != project_root:
                    if (project_root / "scripts").exists():
                        break
                    project_root = project_root.parent
                
                local_bindings_path = str(project_root / "scripts" / "modeldb" / "bindings")
                local_bindings_so = Path(local_bindings_path) / "bindings.cpython-312-x86_64-linux-gnu.so"
                
                if not local_bindings_so.exists():
                    raise FileNotFoundError(f"Local bindings.so not found: {local_bindings_so}")
                
                # å¼ºåˆ¶åŠ è½½æœ¬åœ°bindingsæ¨¡å—
                spec = importlib.util.spec_from_file_location("bindings", local_bindings_so)
                local_bindings = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(local_bindings)
                
                logger.info(f"âœ… Forced loading local bindings from: {local_bindings_so}")
                
                # éªŒè¯å†…æ ¸å‡½æ•°æ˜¯å¦å­˜åœ¨
                if not hasattr(local_bindings, standard_func_name):
                    logger.warning(f"Layer {layer_idx}: Standard kernel {standard_func_name} not found, trying alternative kernels")
                    # å°è¯•æ‰¾åˆ°å¯ç”¨çš„æ ‡å‡†å†…æ ¸
                    available_kernels = [f for f in dir(local_bindings) if 'flash_decoding_allocated_buffer' in f]
                    if available_kernels:
                        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ ‡å‡†å†…æ ¸
                        standard_func_name = available_kernels[0]
                        logger.info(f"Layer {layer_idx}: Using alternative kernel: {standard_func_name}")
                    else:
                        raise AttributeError("No standard kernels available")
                
                standard_kernel_func = getattr(local_bindings, standard_func_name)
                
                # è°ƒç”¨æ ‡å‡†å†…æ ¸
                attn_output = standard_kernel_func(
                    query_states,  # (bs, nh, 1, d)
                    key_codes,     # (bs, nh_k, nk, M)
                    value_codes_standard,  # (bs, nh_k, tokens, M) - è½¬ç½®å›æ ‡å‡†æ ¼å¼
                    key_cents,     # (M, C, d/M)
                    value_cents,   # (M, C, d/M)
                    key_residuals, # (bs, nh_k, r, d)
                    value_residuals, # (bs, nh_k, r, d)
                    residual_length, # r
                    partial_out_buffer,  # partial_out_buffer
                    partial_lse_buffer   # partial_lse_buffer
                )
                
                logger.info(f"Layer {layer_idx}: Standard kernel executed successfully with continuous storage")
                return attn_output
                
            except (AttributeError, ImportError) as e:
                logger.warning(f"Layer {layer_idx}: Standard kernel {standard_func_name} not available: {e}")
                return self._fallback_to_standard_decoding(query_states, layer_idx, residual_length)
                
        except Exception as e:
            logger.error(f"Layer {layer_idx}: Failed to call continuous kernel: {e}")
            logger.error(f"Layer {layer_idx}: Stack trace:", exc_info=True)
            
            # æœ€åçš„fallback
            return self._fallback_to_standard_decoding(query_states, layer_idx, residual_length)
    
    def _fallback_to_standard_decoding(self, query_states, layer_idx, residual_length):
        """å›é€€åˆ°æ ‡å‡†decodingå®ç°"""
        try:
            logger.info(f"Layer {layer_idx}: Using fallback to standard decoding")
            
            # ä½¿ç”¨çœŸå®çš„key_stateså’Œvalue_statesè¿›è¡Œfallback
            real_key_states = self.key_residual_cache[layer_idx][:, :, :residual_length, :]
            real_value_states = self.value_residual_cache[layer_idx][:, :, :residual_length, :]
            
            # è°ƒç”¨çˆ¶ç±»çš„æ ‡å‡†decodingæ–¹æ³•
            return super().decoding(query_states, real_key_states, real_value_states, layer_idx)
            
        except Exception as fallback_e:
            logger.critical(f"Layer {layer_idx}: Fallback decoding failed: {fallback_e}")
            raise
    
    def _fallback_to_standard_decoding(self, query_states, layer_idx, residual_length):
        """
        å›é€€åˆ°æ ‡å‡†è§£ç æ–¹æ³•
        
        Args:
            query_states: Queryå¼ é‡ (bs, nh, 1, d)
            layer_idx: å±‚ç´¢å¼•
            residual_length: æ®‹å·®ç¼“å­˜é•¿åº¦
            
        Returns:
            attention_output: æ³¨æ„åŠ›è¾“å‡º
        """
        logger.warning(f"Layer {layer_idx}: Using fallback standard decoding")
        
        try:
            # è·å–å½“å‰å±‚çš„ç¼“å­˜æ•°æ®
            current_device = query_states.device
            
            # å‡†å¤‡Keyæ•°æ®ï¼šé‡åŒ–ç¼“å­˜ + æ®‹å·®ç¼“å­˜
            key_codes = self.key_cache[layer_idx].to(current_device)  # (bs, nh_k, nk, M)
            key_cents = self.key_cent.to(current_device)  # (M, C, d/M)
            key_residuals = self.key_residual_cache[layer_idx][:, :, :residual_length, :].to(current_device)  # (bs, nh_k, r, d)
            
            # å‡†å¤‡Væ•°æ®ï¼šè¿ç»­å­˜å‚¨çš„è½¬ç½®ç¼“å­˜ + æ®‹å·®ç¼“å­˜
            value_codes = self.value_cache[layer_idx].to(current_device)  # (bs, nh_k, M, tokens) - è¿ç»­å­˜å‚¨çš„è½¬ç½®æ ¼å¼
            value_cents = self.value_cent.to(current_device)  # (M, C, d/M)
            value_residuals = self.value_residual_cache[layer_idx][:, :, :residual_length, :].to(current_device)  # (bs, nh_k, r, d)
            
            # è§£ç Keyå’ŒValue
            from .pq_utils import sa_decode_4d
            
            # è§£ç Key
            key_states = sa_decode_4d(key_codes, key_cents)
            # æ‹¼æ¥æ®‹å·®Key
            if residual_length > 0:
                key_states = torch.cat([key_states, key_residuals], dim=2)
            
            # è§£ç Valueï¼ˆæ³¨æ„ï¼švalue_codesæ˜¯è½¬ç½®å­˜å‚¨çš„ï¼‰
            # éœ€è¦å…ˆè½¬ç½®å›æ¥ï¼Œç„¶åè§£ç 
            value_codes_transposed = value_codes.transpose(2, 3).contiguous()  # (bs, nh_k, tokens, M)
            value_states = sa_decode_4d(value_codes_transposed, value_cents)
            # æ‹¼æ¥æ®‹å·®Value
            if residual_length > 0:
                value_states = torch.cat([value_states, value_residuals], dim=2)
            
            # æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—
            from torch.nn.functional import scaled_dot_product_attention as sdpa
            from transformers.models.llama.modeling_llama import repeat_kv
            
            num_heads = query_states.size(1)
            num_kv_heads = key_states.size(1)
            nrep = num_heads // num_kv_heads
            
            key_states = repeat_kv(key_states, nrep)
            value_states = repeat_kv(value_states, nrep)
            
            attention_output = sdpa(query_states, key_states, value_states, is_causal=True)
            
            logger.debug(f"Layer {layer_idx}: Fallback attention computed successfully")
            return attention_output
            
        except Exception as e:
            logger.error(f"Layer {layer_idx}: Fallback decoding failed: {e}")
            raise
    
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'layer_stats': [],
            'total_memory_usage_mb': 0,
            'memory_breakdown': {}
        }
        
        for layer_idx in range(self.layer_num):
            layer_stat = {
                'layer_idx': layer_idx,
                'seen_tokens': self.seen_tokens[layer_idx],
                'residual_tokens': self.residualed_tokens[layer_idx],
                'key_cache_tokens': self.key_cache[layer_idx].size(2),
                'value_cache_tokens': self.value_cache[layer_idx].size(3)  # è½¬ç½®å­˜å‚¨ï¼Œtokensåœ¨dim=3
            }
            stats['layer_stats'].append(layer_stat)
        
        # å†…å­˜ä½¿ç”¨æ˜ç»†
        total_cache_memory = sum(
            cache.numel() * cache.element_size() 
            for cache in self.key_cache + self.value_cache
        ) / (1024 * 1024)
        
        total_residual_memory = sum(
            cache.numel() * cache.element_size() 
            for cache in self.key_residual_cache + self.value_residual_cache
        ) / (1024 * 1024)
        
        total_prefill_residual_memory = sum(
            cache.numel() * cache.element_size() 
            for cache in getattr(self, 'key_prefill_residual', []) + getattr(self, 'value_prefill_residual', [])
        ) / (1024 * 1024)
        
        stats['memory_breakdown'] = {
            'cache_memory_mb': total_cache_memory,
            'residual_memory_mb': total_residual_memory,
            'prefill_residual_memory_mb': total_prefill_residual_memory,
            'total_memory_mb': total_cache_memory + total_residual_memory + total_prefill_residual_memory
        }
        
        return stats
    
    def get_performance_stats(self) -> Dict:
        """è·å–è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_cache_stats()
        
        # è®¡ç®—å†…å­˜ä½¿ç”¨æ•ˆç‡
        total_tokens = sum(layer['key_cache_tokens'] + layer['value_cache_tokens'] for layer in stats['layer_stats'])
        memory_efficiency = (stats['memory_breakdown']['total_memory_mb'] / 
                           (total_tokens * self.M * 2 / (1024 * 1024)) * 100) if total_tokens > 0 else 0
        
        # æ€§èƒ½è¯„åˆ†ï¼ˆ0-100ï¼‰
        performance_score = 0
        if memory_efficiency > 80:
            performance_score += 50
        elif memory_efficiency > 60:
            performance_score += 40
        else:
            performance_score += 30
            
        # åŸºäºç¼“å­˜ä½¿ç”¨æƒ…å†µçš„è¯„åˆ†
        cache_utilization = total_tokens / (self.layer_num * 1000) * 100  # å‡è®¾æ¯å±‚1000ä¸ªtokenä¸ºåŸºå‡†
        if cache_utilization > 80:
            performance_score += 50
        elif cache_utilization > 60:
            performance_score += 40
        else:
            performance_score += 30
        
        performance_stats = {
            'memory_efficiency_percent': memory_efficiency,
            'cache_utilization_percent': cache_utilization,
            'performance_score': performance_score,
            'recommendations': []
        }
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        if memory_efficiency < 60:
            performance_stats['recommendations'].append("å†…å­˜ä½¿ç”¨æ•ˆç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–ç¼“å­˜å¤§å°å’Œå†…å­˜ç®¡ç†")
        
        if cache_utilization < 60:
            performance_stats['recommendations'].append("ç¼“å­˜åˆ©ç”¨ç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–åºåˆ—é•¿åº¦å’Œç¼“å­˜ç­–ç•¥")
        
        if performance_score < 50:
            performance_stats['recommendations'].append("æ•´ä½“æ€§èƒ½è¾ƒå·®ï¼Œå»ºè®®å…¨é¢ä¼˜åŒ–ç¼“å­˜ç®¡ç†å’Œå†…å­˜ä½¿ç”¨")
        
        stats['performance_analysis'] = performance_stats
        return stats
    
    def print_performance_summary(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡æ‘˜è¦"""
        stats = self.get_performance_stats()
        
        print("\n" + "="*60)
        print("ğŸš€ PagedPQCache æ€§èƒ½ç»Ÿè®¡æ‘˜è¦")
        print("="*60)
        
        # ç¼“å­˜ä½¿ç”¨æƒ…å†µ
        print(f"ğŸ“Š ç¼“å­˜ä½¿ç”¨æƒ…å†µ:")
        total_k_tokens = sum(layer['key_cache_tokens'] for layer in stats['layer_stats'])
        total_v_tokens = sum(layer['value_cache_tokens'] for layer in stats['layer_stats'])
        print(f"   KçŸ©é˜µtokens: {total_k_tokens}")
        print(f"   VçŸ©é˜µtokens: {total_v_tokens}")
        print(f"   æ€»tokens: {total_k_tokens + total_v_tokens}")
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_stats = stats['memory_breakdown']
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨æƒ…å†µ:")
        print(f"   ç¼“å­˜å†…å­˜: {memory_stats['cache_memory_mb']:.2f} MB")
        print(f"   æ®‹å·®ç¼“å­˜: {memory_stats['residual_memory_mb']:.2f} MB")
        print(f"   æ€»å†…å­˜: {memory_stats['total_memory_mb']:.2f} MB")
        
        # æ€§èƒ½åˆ†æ
        perf_stats = stats['performance_analysis']
        print(f"\nâš¡ æ€§èƒ½åˆ†æ:")
        print(f"   å†…å­˜ä½¿ç”¨æ•ˆç‡: {perf_stats['memory_efficiency_percent']:.1f}%")
        print(f"   ç¼“å­˜åˆ©ç”¨ç‡: {perf_stats['cache_utilization_percent']:.1f}%")
        print(f"   æ€§èƒ½è¯„åˆ†: {perf_stats['performance_score']}/100")
        
        # æ”¹è¿›å»ºè®®
        if perf_stats['recommendations']:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(perf_stats['recommendations'], 1):
                print(f"   {i}. {rec}")
        
        print("="*60)
    
    def show_current_status(self):
        """æ˜¾ç¤ºå½“å‰ç¼“å­˜çŠ¶æ€"""
        stats = self.get_cache_stats()
        
        print("\n" + "="*50)
        print("ğŸ“Š PagedPQCache å½“å‰çŠ¶æ€")
        print("="*50)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ”§ åŸºæœ¬ä¿¡æ¯:")
        print(f"   å±‚æ•°: {self.layer_num}")
        print(f"   æ‰¹æ¬¡å¤§å°: {self.bs}")
        print(f"   KVå¤´æ•°: {self.num_key_value_heads}")
        print(f"   å­ç©ºé—´æ•°: {self.M}")
        print(f"   é¡µé¢å¤§å°: {self.page_size} tokens")
        
        # ç¼“å­˜ä½¿ç”¨æƒ…å†µ
        print(f"\nğŸ“„ ç¼“å­˜ä½¿ç”¨æƒ…å†µ:")
        for layer_idx in range(self.layer_num):
            layer_stats = stats['layer_stats'][layer_idx]
            print(f"   å±‚ {layer_idx}: K={layer_stats['key_cache_tokens']} tokens, V={layer_stats['value_cache_tokens']} tokens, "
                  f"æ®‹å·®={layer_stats['residual_tokens']} tokens")
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_stats = stats['memory_breakdown']
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨æƒ…å†µ:")
        print(f"   ç¼“å­˜å†…å­˜: {memory_stats['cache_memory_mb']:.2f} MB")
        print(f"   æ®‹å·®ç¼“å­˜: {memory_stats['residual_memory_mb']:.2f} MB")
        print(f"   æ€»å†…å­˜: {memory_stats['total_memory_mb']:.2f} MB")
        
        # æ€§èƒ½æŒ‡æ ‡
        if 'performance_analysis' in stats:
            perf_stats = stats['performance_analysis']
            print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
            print(f"   å†…å­˜ä½¿ç”¨æ•ˆç‡: {perf_stats['memory_efficiency_percent']:.1f}%")
            print(f"   ç¼“å­˜åˆ©ç”¨ç‡: {perf_stats['cache_utilization_percent']:.1f}%")
            print(f"   æ€§èƒ½è¯„åˆ†: {perf_stats['performance_score']}/100")
        
        print("="*50)
    
    def get_memory_usage_summary(self) -> str:
        """è·å–å†…å­˜ä½¿ç”¨æ‘˜è¦"""
        stats = self.get_cache_stats()
        memory_stats = stats['memory_breakdown']
        
        summary = (
            f"å†…å­˜ä½¿ç”¨æ‘˜è¦:\n"
            f"  ğŸ“Š æ€»å†…å­˜: {memory_stats['total_memory_mb']:.2f} MB\n"
            f"  ğŸ”§ ç¼“å­˜: {memory_stats['cache_memory_mb']:.2f} MB\n"
            f"  ğŸ“ æ®‹å·®: {memory_stats['residual_memory_mb']:.2f} MB\n"
            f"  âš¡ é¢„å¡«å……: {memory_stats['prefill_residual_memory_mb']:.2f} MB"
        )
        return summary
    

    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("Cleaning up PagedPQCache resources")
        
        # ç»Ÿè®¡æ¸…ç†å‰çš„èµ„æºä½¿ç”¨æƒ…å†µ
        stats_before = self.get_cache_stats()
        total_memory_before = stats_before['memory_breakdown']['total_memory_mb']
        
        logger.info(f"Before cleanup: {total_memory_before:.2f} MB")
        
        # æ¸…ç†ç¼“å­˜æ•°æ®
        for layer_idx in range(self.layer_num):
            # æ¸…ç©ºKå’ŒVç¼“å­˜
            self.key_cache[layer_idx] = torch.zeros((self.bs, self.num_key_value_heads, 0, self.M), dtype=self.dtype, device='cuda')
            self.value_cache[layer_idx] = torch.zeros((self.bs, self.num_key_value_heads, self.M, 0), dtype=self.dtype, device='cuda')
            
            # é‡ç½®è®¡æ•°å™¨
            self.seen_tokens[layer_idx] = 0
            self.residualed_tokens[layer_idx] = 0
            
            # æ¸…ç©ºæ®‹å·®ç¼“å­˜
            self.key_residual_cache[layer_idx].fill_(0)
            self.value_residual_cache[layer_idx].fill_(0)
        
        # ç»Ÿè®¡æ¸…ç†åçš„èµ„æºä½¿ç”¨æƒ…å†µ
        stats_after = self.get_cache_stats()
        total_memory_after = stats_after['memory_breakdown']['total_memory_mb']
        
        # è®¡ç®—é‡Šæ”¾çš„èµ„æº
        memory_freed = total_memory_before - total_memory_after
        
        logger.info(f"After cleanup: {total_memory_after:.2f} MB")
        logger.info(f"Cleanup completed: Freed {memory_freed:.2f} MB")
        
        # è¾“å‡ºæ€§èƒ½ç»Ÿè®¡æ‘˜è¦
        # self.print_performance_summary()
    
    def _compute_attention_fallback(self, query_states, key_states, value_states, layer_idx):
        """
        fallback attentionè®¡ç®—
        
        Args:
            query_states: Queryå¼ é‡
            key_states: Keyå¼ é‡
            value_states: Valueå¼ é‡
            layer_idx: å±‚ç´¢å¼•
            
        Returns:
            attention_output: æ³¨æ„åŠ›è¾“å‡º
        """
        logger.info(f"Layer {layer_idx}: Using fallback attention computation")
        
        # å¦‚æœéœ€è¦distort_recentï¼Œåé‡åŒ–ç”¨äºattentionè®¡ç®—
        if hasattr(self, 'distort_recent') and self.distort_recent:
            with Timer("PagedPQCache.prefill.decode_fallback"):
                key_states = sa_decode_4d(self.key_cache[layer_idx], self.key_cent)
                value_states = sa_decode_4d(self.value_cache[layer_idx], self.value_cent)
                torch.cuda.synchronize()
        
        # æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—
        with Timer("PagedPQCache.prefill.attention_fallback"):
            from torch.nn.functional import scaled_dot_product_attention as sdpa
            from transformers.models.llama.modeling_llama import repeat_kv

            num_heads = query_states.size(1)
            num_kv_heads = key_states.size(1)
            nrep = num_heads // num_kv_heads

            key_states = repeat_kv(key_states, nrep)
            value_states = repeat_kv(value_states, nrep)
            
            attention_output = sdpa(query_states, key_states, value_states, is_causal=True)
            
            logger.debug(f"PagedPQCache fallback attention completed: layer={layer_idx}")
            
            return attention_output