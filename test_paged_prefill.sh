#!/bin/bash
# æµ‹è¯• PagedPQCache çš„ prefill æ–¹æ³•

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m'

echo -e "${BLUE}ğŸ§ª æµ‹è¯• PagedPQCache çš„ prefill æ–¹æ³•${NC}"
echo "=============================================="

# è®¾ç½®ç¯å¢ƒ
export LD_LIBRARY_PATH="/root/miniconda3/envs/million/lib/python3.12/site-packages/torch/lib:$LD_LIBRARY_PATH"
PYTHON_PATH="/root/miniconda3/envs/million/bin/python"

echo -e "${YELLOW}ğŸ”§ æµ‹è¯•é…ç½®:${NC}"
echo "  æ¨¡å‹: llama-2-7b.json"
echo "  æ•°æ®é›†: _synthetic"
echo "  å‚æ•°: -M 32 --nbits 8 --paged --page_size 64 --extended_residual 128 --max_pages 3000"
echo "  ç›®æ ‡: éªŒè¯ prefill æ–¹æ³•æ˜¯å¦æ­£ç¡®è§¦å‘é¡µé¢åˆ†é…"
echo "  ä¿®å¤: å¢åŠ max_pagesåˆ°3000ï¼Œé¿å…é¡µé¢æ± è€—å°½"
echo

# æ£€æŸ¥ç¯å¢ƒ
if [ ! -f "$PYTHON_PATH" ]; then
    echo -e "${RED}âŒ Pythonç¯å¢ƒä¸å­˜åœ¨: $PYTHON_PATH${NC}"
    exit 1
fi

echo -e "${CYAN}ğŸš€ å¼€å§‹æµ‹è¯•...${NC}"

# è¿è¡Œæµ‹è¯•
$PYTHON_PATH -m scripts.modeldb.main_pq \
    -f llama-2-7b.json \
    -p evaluation \
    -d _synthetic \
    --merged_training \
    --half \
    -M 32 \
    --nbits 8 \
    --paged \
    --page_size 64 \
    --extended_residual 128 \
    --max_pages 3000

echo
echo -e "${GREEN}ğŸ‰ æµ‹è¯•å®Œæˆï¼${NC}"
echo -e "${CYAN}ğŸ’¡ æ£€æŸ¥è¾“å‡ºä¸­çš„é¡µé¢åˆ†é…æƒ…å†µï¼š${NC}"
echo "â€¢ åº”è¯¥çœ‹åˆ° 'Total pages allocated: > 0'"
echo "â€¢ åº”è¯¥çœ‹åˆ° 'Layer X: Y tokens, Z pages'"
echo "â€¢ åº”è¯¥çœ‹åˆ° prefill ç›¸å…³çš„è°ƒè¯•ä¿¡æ¯"

